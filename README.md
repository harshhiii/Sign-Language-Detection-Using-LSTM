Sign Language Detection using LSTM

This project focuses on real-time sign language recognition using deep learning and computer vision techniques. It begins with extracting MediaPipe Holistic keypoints from video sequences to represent gestures as numerical data. These sequences are then processed through a TensorFlow/Keras-based LSTM model designed to learn and decode temporal patterns of movements. Finally, the trained model is applied for real-time sign language prediction, enabling accurate recognition of gestures directly from live video input.
